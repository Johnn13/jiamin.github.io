# Chapter1 çº¿æ€§å›å½’

é—®é¢˜å®šä¹‰ï¼šç»™å®šä¸€ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯»æ‰¾æ¨¡å‹çš„æƒé‡$\mathbf{w}$å’Œåç½®$b$ã€‚

å½“æˆ‘ä»¬çš„è¾“å…¥åŒ…å« $d$ ä¸ªç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬å°†é¢„æµ‹ç»“æœ $\hat{y}$ï¼Œï¼ˆé€šå¸¸ä½¿ç”¨â€œå°–è§’â€ç¬¦å·è¡¨ç¤º $y$ çš„ä¼°è®¡å€¼ï¼‰è¡¨ç¤ºä¸ºï¼š

$$ \hat{y} = w_1Â  x_1 + ... + w_dÂ  x_d + b $$

å°†æ‰€æœ‰ç‰¹å¾æ”¾åˆ°å‘é‡  $\mathbf{x} \in \mathbb{R}^d$ ä¸­ï¼Œå¹¶å°†æ‰€æœ‰æƒé‡æ”¾åˆ°å‘é‡ $\mathbf{w} \in \mathbb{R}^d$ ä¸­ï¼Œé€šè¿‡ç‚¹ç§¯å¯ä»¥è¡¨ç¤ºæ¨¡å‹ï¼š

$$ \hat{y} = \mathbf{w}^\top \mathbf{x} + b $$

å‘é‡$\mathbf{x}$å¯¹åº”äºå•ä¸ªæ•°æ®æ ·æœ¬çš„ç‰¹å¾ã€‚è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬è€ƒè™‘å¤šä¸ªæ ·æœ¬ç‚¹çš„æƒ…å†µã€‚ç”¨ç¬¦å·è¡¨ç¤ºçš„çŸ©é˜µ$\mathbf{X} \in \mathbb{R}^{n \times d}$ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿åœ°å¼•ç”¨æˆ‘ä»¬æ•´ä¸ªæ•°æ®é›†çš„$n$ä¸ªæ ·æœ¬ã€‚å…¶ä¸­ï¼Œ$\mathbf{X}$çš„æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—æ˜¯ä¸€ç§ç‰¹å¾ã€‚

å¯¹äºç‰¹å¾é›†åˆ$\mathbf{X}$ï¼Œé¢„æµ‹å€¼$\hat{\mathbf{y}} \in \mathbb{R}^n$å¯ä»¥é€šè¿‡çŸ©é˜µ-å‘é‡ä¹˜æ³•è¡¨ç¤ºä¸ºï¼š

$$ {\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b $$

ç»™å®šè®­ç»ƒæ•°æ®ç‰¹å¾$\mathbf{X}$å’Œå¯¹åº”çš„å·²çŸ¥æ ‡ç­¾$\mathbf{y}$ï¼Œçº¿æ€§å›å½’çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„æƒé‡å‘é‡$\mathbf{w}$å’Œåç½®$b$ä½¿å¾—æ–°æ ·æœ¬é¢„æµ‹æ ‡ç­¾çš„è¯¯å·®å°½å¯èƒ½å°ã€‚

# æŸå¤±å‡½æ•°ï¼ˆLoss functionï¼‰

å›å½’é—®é¢˜ä¸­æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°æ˜¯å¹³æ–¹è¯¯å·®å‡½æ•°ï¼Œå½“æ ·æœ¬$i$çš„é¢„æµ‹å€¼ä¸º$\hat{y}^{(i)}$ï¼Œå…¶ç›¸åº”çš„çœŸå®æ ‡ç­¾ä¸º$y^{(i)}$æ—¶ï¼Œå¹³æ–¹è¯¯å·®å¯ä»¥å®šä¹‰ä¸ºä»¥ä¸‹å…¬å¼ï¼š

$$ l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2 $$

å¸¸æ•°$\frac{1}{2}$ä¸ä¼šå¸¦æ¥æœ¬è´¨çš„å·®åˆ«ï¼Œä½†è¿™æ ·åœ¨å½¢å¼ä¸Šç¨å¾®ç®€å•ä¸€äº›ï¼ˆå› ä¸ºå½“æˆ‘ä»¬å¯¹æŸå¤±å‡½æ•°æ±‚å¯¼åå¸¸æ•°ç³»æ•°ä¸º$1$ï¼‰ï¼Œå¸¦å…¥${\hat{\mathbf{y}}}$æ±‚å¹³å‡å¾—åˆ°ï¼š

$$ L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2 $$

åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›å¯»æ‰¾ä¸€ç»„å‚æ•°ï¼ˆ$\mathbf{w}^*, b^*$ï¼‰ï¼Œè¿™ç»„å‚æ•°èƒ½æœ€å°åŒ–åœ¨æ‰€æœ‰è®­ç»ƒæ ·æœ¬ä¸Šçš„æ€»æŸå¤±ã€‚å¦‚ä¸‹å¼ï¼šè§£æè§£

$$
\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\ L(\mathbf{w}, b)
$$

# è§£æè§£

çº¿æ€§å›å½’çš„è§£å¯ä»¥ç”¨ä¸€ä¸ªå…¬å¼ç®€å•åœ°è¡¨è¾¾å‡ºæ¥ï¼Œè¿™ç±»è§£å«åšè§£æè§£ï¼ˆanalytical solutionï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†åç½®$b$åˆå¹¶åˆ°å‚æ•°$\mathbf{w}$ä¸­ï¼Œåˆå¹¶æ–¹æ³•æ˜¯åœ¨åŒ…å«æ‰€æœ‰å‚æ•°çš„çŸ©é˜µä¸­é™„åŠ ä¸€åˆ—ã€‚

æˆ‘ä»¬çš„é¢„æµ‹é—®é¢˜æ˜¯æœ€å°åŒ– $\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})=\frac{1}{2 n}\|\mathbf{y}-\mathbf{X w}\|^{2}$ã€‚æˆ‘ä»¬å¯¹å…¶æ±‚åå¯¼å¾—åˆ°ï¼š

$$
Â \frac{\partial}{\partial \mathbf{w}} \ell(\mathbf{X}, \mathbf{y}, \mathbf{w})=\frac{1}{n}(\mathbf{y}-\mathbf{X w})^{T} \mathbf{X}
$$

æŸå¤±å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼Œå› æ­¤åœ¨æŸå¤±å¹³é¢ä¸Šåªæœ‰ä¸€ä¸ªä¸´ç•Œç‚¹ï¼Œè¿™ä¸ªä¸´ç•Œç‚¹å¯¹åº”äºæ•´ä¸ªåŒºåŸŸçš„æŸå¤±æå°ç‚¹ã€‚å°†æŸå¤±å…³äº$\mathbf{w}$çš„å¯¼æ•°è®¾ä¸º0ï¼Œå¾—åˆ°è§£æè§£ï¼š

$$
\begin{align} 
& \frac{\partial}{\partial \mathbf{w}} \ell(\mathbf{X}, \mathbf{y}, \mathbf{w})=0 \\
&\Leftrightarrow   \frac{1}{n}(\mathbf{y}-\mathbf{X w})^{T} \mathbf{X}= 0\\
&\Leftrightarrow \mathbf{w}^* = (\mathbf{X }^{T}\mathbf{X })^{-1}\mathbf{X }\mathbf{y}
\end{align}
$$

<aside>
ğŸ’¡

åƒçº¿æ€§å›å½’è¿™æ ·çš„ç®€å•é—®é¢˜å­˜åœ¨è§£æè§£ï¼Œä½†å¹¶ä¸æ˜¯æ‰€æœ‰çš„é—®é¢˜éƒ½å­˜åœ¨è§£æè§£ã€‚ è§£æè§£å¯ä»¥è¿›è¡Œå¾ˆå¥½çš„æ•°å­¦åˆ†æï¼Œä½†è§£æè§£å¯¹é—®é¢˜çš„é™åˆ¶å¾ˆä¸¥æ ¼ï¼Œå¯¼è‡´å®ƒæ— æ³•å¹¿æ³›åº”ç”¨åœ¨æ·±åº¦å­¦ä¹ é‡Œã€‚

</aside>

æ€»ç»“ï¼š

- çº¿æ€§å›å½’æ˜¯å¯¹ n ç»´è¾“å…¥çš„åŠ æƒï¼Œå¤–åŠ åå·®
- ä½¿ç”¨å¹³æ–¹æŸå¤±æ¥è¡¡é‡é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®å¼‚
- çº¿æ€§å›å½’æœ‰æ˜¾ç¤ºè§£
- çº¿æ€§å›å½’å¯ä»¥çœ‹ä½œæ˜¯å•å±‚ç¥ç»ç½‘ç»œ

# åŸºç¡€ä¼˜åŒ–ç®—æ³•

å¦‚ä½•å¿«é€Ÿåœ°æ±‚å‡ºï¼ˆ$\mathbf{w}^*, b^*$ï¼‰çš„ä¸€äº›ç®—æ³•

## æ¢¯åº¦ä¸‹é™

æ¢¯åº¦ä¸‹é™ï¼ˆgradient descentï¼‰æ–¹æ³•å‡ ä¹å¯ä»¥ä¼˜åŒ–æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å®ƒé€šè¿‡ä¸æ–­åœ°åœ¨æŸå¤±å‡½æ•°é€’å‡çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°æ¥å‡ä½è¯¯å·®ã€‚

> æ¢¯åº¦ä¸‹é™æœ€ç®€å•çš„æ–¹æ³•æ˜¯è®¡ç®—æŸå¤±å‡½æ•°ï¼ˆæ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬çš„æŸå¤±å‡å€¼ï¼‰å…³äºæ¨¡å‹å‚æ•°çš„åå¯¼æ•°ï¼ˆåˆç§°ä¸ºæ¢¯åº¦ï¼‰ã€‚
> 

![è“è‰²æ˜¯æŸå¤±å‡½æ•°çš„ç­‰é«˜çº¿](Chapter1%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%202054fd39d708806a97a8ed3bb251792e/image.png)

è“è‰²æ˜¯æŸå¤±å‡½æ•°çš„ç­‰é«˜çº¿

æ¢¯åº¦ä¸‹é™çš„æ­¥éª¤ï¼š

1. æŒ‘é€‰ä¸€ä¸ªåˆå§‹å€¼ $\mathbf{w}_0$
2. é‡å¤è¿­ä»£å‚æ•° $t =1,2,3,\cdots$
    
    $$
    \mathbf{w}_{t}=\mathbf{w}_{t-1}-\eta \frac{\partial \ell}{\partial \mathbf{w}_{t-1}}
    $$
    
    - æ²¿æ¢¯åº¦æ–¹å‘æ˜¯å¢åŠ æŸå¤±å‡½æ•°$\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})$çš„å€¼ï¼Œå› æ­¤åœ¨å¯¼æ•°å‰é¢åŠ äº†è´Ÿå·ï¼›
    - å­¦ä¹ ç‡$\eta$ï¼šæ­¥é•¿çš„è¶…å‚æ•°ã€‚å­¦ä¹ ç‡ä¸èƒ½å¤ªå°ï¼Œä¹Ÿä¸èƒ½å¤ªå¤§ã€‚å‚è€ƒï¼š[1.3 å­¦ä¹ ç‡ $\alpha$](https://www.notion.so/1-3-alpha-2004fd39d7088045a74bd83af5e41568?pvs=21)

## å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™

åœ¨å®é™…ä¸­ï¼Œæ¯æ¬¡è®¡ç®— $\mathbf{w}$ çš„æ¢¯åº¦éƒ½å¿…é¡»éå†æ•´ä¸ªæ•°æ®é›†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåœ¨æ¯æ¬¡éœ€è¦è®¡ç®—æ›´æ–°çš„æ—¶å€™éšæœºæŠ½å–ä¸€å°æ‰¹æ ·æœ¬ï¼Œ è¿™ç§å˜ä½“å«åš***å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™***ï¼ˆminibatch stochastic gradient descentï¼‰ã€‚

åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆéšæœºæŠ½æ ·ä¸€ä¸ªå°æ‰¹é‡ $\mathcal{B}$ï¼Œå®ƒæ˜¯ç”±å›ºå®šæ•°é‡çš„è®­ç»ƒæ ·æœ¬ç»„æˆçš„ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—å°æ‰¹é‡çš„å¹³å‡æŸå¤±å…³äºæ¨¡å‹å‚æ•°çš„å¯¼æ•°ï¼ˆä¹Ÿå¯ä»¥ç§°ä¸ºæ¢¯åº¦ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ¢¯åº¦ä¹˜ä»¥ä¸€ä¸ªé¢„å…ˆç¡®å®šçš„æ­£æ•°$\eta$ï¼Œå¹¶ä»å½“å‰å‚æ•°çš„å€¼ä¸­å‡æ‰ã€‚æˆ‘ä»¬ç”¨ä¸‹é¢çš„æ•°å­¦å…¬å¼æ¥è¡¨ç¤ºè¿™ä¸€æ›´æ–°è¿‡ç¨‹ï¼ˆ$\partial$è¡¨ç¤ºåå¯¼æ•°ï¼‰ï¼š

$$
(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)
$$

æ€»ç»“ä¸€ä¸‹ï¼Œç®—æ³•çš„æ­¥éª¤å¦‚ä¸‹ï¼š 

1. åˆå§‹åŒ–æ¨¡å‹å‚æ•°çš„å€¼ï¼Œå¦‚éšæœºåˆå§‹åŒ–ï¼› 
2. ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–å°æ‰¹é‡æ ·æœ¬ä¸”åœ¨è´Ÿæ¢¯åº¦çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°ï¼Œå¹¶ä¸æ–­è¿­ä»£è¿™ä¸€æ­¥éª¤ã€‚ å¯¹äºå¹³æ–¹æŸå¤±å’Œä»¿å°„å˜æ¢ï¼Œæˆ‘ä»¬å¯ä»¥æ˜ç¡®åœ°å†™æˆå¦‚ä¸‹å½¢å¼:
    
    $$
    \begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -Â  Â \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -Â  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)Â  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}
    $$
    

å…¶ä¸­ï¼Œ $\mathbf{w}$  å’Œ  $\mathbf{x}$  éƒ½æ˜¯å‘é‡ã€‚${|\mathcal{B}|}$ è¡¨ç¤ºæ¯ä¸ªå°æ‰¹é‡ä¸­çš„æ ·æœ¬æ•°ï¼Œè¿™ä¹Ÿæˆä¸ºæ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰ã€‚$\eta$ è¡¨ç¤ºå­¦ä¹ ç‡ï¼ˆlearning rateï¼‰ã€‚æ‰¹é‡å¤§å°å’Œå­¦ä¹ ç‡çš„å€¼é€šå¸¸æ˜¯æ‰‹åŠ¨é¢„å…ˆæŒ‡å®šï¼Œè€Œä¸æ˜¯é€šè¿‡æ¨¡å‹è®­ç»ƒå¾—åˆ°çš„ã€‚è¿™äº›å¯ä»¥è°ƒæ•´ä½†ä¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°çš„å‚æ•°ç§°ä¸º*è¶…å‚æ•°*ï¼ˆhyperparameterï¼‰ã€‚*è°ƒå‚*ï¼ˆhyperparameter tuningï¼‰æ˜¯é€‰æ‹©è¶…å‚æ•°çš„è¿‡ç¨‹ã€‚è¶…å‚æ•°é€šå¸¸æ˜¯æˆ‘ä»¬æ ¹æ®è®­ç»ƒè¿­ä»£ç»“æœæ¥è°ƒæ•´çš„ï¼Œè€Œè®­ç»ƒè¿­ä»£ç»“æœæ˜¯åœ¨ç‹¬ç«‹çš„*éªŒè¯æ•°æ®é›†*ï¼ˆvalidation datasetï¼‰ä¸Šè¯„ä¼°å¾—åˆ°çš„ã€‚

åœ¨è®­ç»ƒäº†é¢„å…ˆç¡®å®šçš„è‹¥å¹²è¿­ä»£æ¬¡æ•°åï¼ˆæˆ–è€…ç›´åˆ°æ»¡è¶³æŸäº›å…¶ä»–åœæ­¢æ¡ä»¶åï¼‰ï¼Œæˆ‘ä»¬è®°å½•ä¸‹æ¨¡å‹å‚æ•°çš„ä¼°è®¡å€¼ï¼Œè¡¨ç¤ºä¸º$\hat{\mathbf{w}}, \hat{b}$ã€‚ä½†æ˜¯ï¼Œå³ä½¿æˆ‘ä»¬çš„å‡½æ•°ç¡®å®æ˜¯çº¿æ€§çš„ä¸”æ— å™ªå£°ï¼Œè¿™äº›ä¼°è®¡å€¼ä¹Ÿä¸ä¼šä½¿æŸå¤±å‡½æ•°çœŸæ­£åœ°è¾¾åˆ°æœ€å°å€¼ã€‚å› ä¸ºç®—æ³•ä¼šä½¿å¾—æŸå¤±å‘æœ€å°å€¼ç¼“æ…¢æ”¶æ•›ï¼Œä½†å´ä¸èƒ½åœ¨æœ‰é™çš„æ­¥æ•°å†…éå¸¸ç²¾ç¡®åœ°è¾¾åˆ°æœ€å°å€¼ã€‚

## ä»£ç å®ç°1

```python
# çº¿æ€§å›å½’â€”â€”ä»é›¶å¼€å§‹å®ç°
import random
import torch 
from d2l import torch as d2l

def synthetic_data(w, b, num_examples):
    """ç”Ÿæˆy = Xw + b + å™ªå£°"""

    # ç”Ÿæˆå‡å€¼=0,æ–¹å·®=1,æ ·æœ¬æ•°ä¸º{num_examples},ç‰¹è¯æ•°=wçš„é•¿åº¦çš„ä¸€ç»„éšæœºæ ·æœ¬
    X = torch.normal(0, 1, (num_examples, len(w))) 
    # y = Xw + b
    y = torch.matmul(X, w) + b
    # åŠ å…¥éšæœºå™ªå£°
    y += torch.normal(0, 0.01, y.shape)
    # æŠŠX,å’Œyåšæˆä¸€ä¸ªåˆ—å‘é‡è¿”å›
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)

print('features:', features[0], '\nlabels:', labels[0])

# d2l.set_figsize()
# d2l.plt.scatter(features[:, 1].detach().numpy(),
#                 labels.detach().numpy(), 1);

def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # è¿™äº›æ ·æœ¬æ˜¯éšæœºè¯»å–çš„ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡ºåº
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):    
        batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]

batch_size = 10
 
for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break

# å®šä¹‰åˆå§‹åŒ–æ¨¡å‹å‚æ•°
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# å®šä¹‰æ¨¡å‹
def linreg(X, w, b):
    """çº¿æ€§å›å½’æ¨¡å‹"""
    return torch.matmul(X, w) + b

# å®šä¹‰æŸå¤±å‡½æ•°
def squared_loss(y_hat, y):
    """å‡æ–¹æŸå¤±"""
    return (y_hat - y.reshape(y_hat.shape))**2 / 2

# å®šä¹‰ä¼˜åŒ–ç®—æ³•
"""
    inputs: 
        params: w, b
        lr: learning rate
        batch_size: the number of sample
"""
def sgd(params, lr, batch_size):
    """å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()

# è®­ç»ƒè¿‡ç¨‹
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        # step1: forward pass
        l = loss(net(X, w, b), y)
        # å› ä¸º l çš„å½¢çŠ¶æ˜¯ batch_size*1, è€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡
        # back propagation
        l.sum().backward()
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')

print(f'wçš„ä¼°è®¡è¯¯å·®: {true_w - w.reshape(true_w.shape)}')
print(f'bçš„ä¼°è®¡è¯¯å·®: {true_b - b}') 
```

## ä»£ç å®ç°2ï¼ˆç®€æ´ç‰ˆæœ¬ï¼‰

```python
# çº¿æ€§å›å½’çš„ç®€æ´å®ç°
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

"""
å°†featureså’Œlabelsä½œä¸ºAPIçš„å‚æ•°ä¼ é€’,
å¹¶é€šè¿‡æ•°æ®è¿­ä»£å™¨æŒ‡å®šbatch_sizeã€‚
å¸ƒå°”å€¼is_trainè¡¨ç¤ºæ˜¯å¦å¸Œæœ›æ•°æ®è¿­ä»£å™¨å¯¹è±¡åœ¨æ¯ä¸ªè¿­ä»£å‘¨æœŸå†…æ‰“ä¹±æ•°æ®ã€‚"""
def load_array(data_arrays, batch_size, is_train=True):
    """ æ„é€ ä¸€ä¸ªPyTorchæ•°æ®è¿­ä»£å™¨"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)

# step1: å®šä¹‰æ¨¡å‹
# æ€è·¯: é¦–å…ˆå®šä¹‰ä¸€ä¸ªæ¨¡å‹å˜é‡ netï¼Œå®ƒæ˜¯ä¸€ä¸ª Sequential ç±»çš„å®ä¾‹ã€‚Sequential ç±»å°†å¤šä¸ªå±‚ä¸²è”åœ¨ä¸€èµ·ã€‚
# å½“ç»™å®šè¾“å…¥æ•°æ®æ—¶ï¼ŒSequential å®ä¾‹å°†æ•°æ®ä¼ å…¥åˆ°ç¬¬ä¸€å±‚ï¼Œç„¶åå°†ç¬¬ä¸€å±‚çš„è¾“å‡ºä½œä¸ºç¬¬äºŒå±‚çš„è¾“å…¥ï¼Œä»¥æ­¤ç±»æ¨

# åœ¨PyTorchä¸­ï¼Œå…¨è¿æ¥å±‚åœ¨Linearç±»ä¸­å®šä¹‰ã€‚ 
# å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å°†ä¸¤ä¸ªå‚æ•°ä¼ é€’åˆ°nn.Linearä¸­ã€‚ 
# ç¬¬ä¸€ä¸ªæŒ‡å®šè¾“å…¥ç‰¹å¾å½¢çŠ¶ï¼Œå³2ï¼Œx = [x1, x2]
# ç¬¬äºŒä¸ªæŒ‡å®šè¾“å‡ºç‰¹å¾å½¢çŠ¶ï¼Œè¾“å‡ºç‰¹å¾å½¢çŠ¶ä¸ºå•ä¸ªæ ‡é‡ï¼Œå› æ­¤ä¸º1ã€‚y = [y1]
from torch import nn

net = nn.Sequential(nn.Linear(2, 1))

# step2: åˆå§‹åŒ–æ¨¡å‹å‚æ•°
# åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŒ‡å®šæ¯ä¸ªæƒé‡å‚æ•°åº”è¯¥ä»å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒä¸­éšæœºé‡‡æ ·ï¼Œ 
# åç½®å‚æ•°å°†åˆå§‹åŒ–ä¸ºé›¶ã€‚

# é€šè¿‡ net[0] é€‰æ‹©ç½‘ç»œä¸­çš„ç¬¬ä¸€ä¸ªå›¾å±‚
# ä½¿ç”¨weight.dataå’Œbias.dataæ–¹æ³•è®¿é—®å‚æ•°ã€‚ 
# ä½¿ç”¨æ›¿æ¢æ–¹æ³•normal_å’Œfill_æ¥é‡å†™å‚æ•°å€¼
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
# print(net[0].weight.data, net[0].bias.data)

# step3: å®šä¹‰æŸå¤±å‡½æ•°
# [è®¡ç®—å‡æ–¹è¯¯å·®ä½¿ç”¨çš„æ˜¯MSELossç±»ï¼Œä¹Ÿç§°ä¸ºå¹³æ–¹L2èŒƒæ•°]ã€‚
# é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒè¿”å›æ‰€æœ‰æ ·æœ¬æŸå¤±çš„å¹³å‡å€¼
loss = nn.MSELoss()

# step4: å®šä¹‰ä¼˜åŒ–ç®—æ³•
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

# step5: è®­ç»ƒ
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X), y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

# æ¯”è¾ƒè®­ç»ƒåçš„[w, b]ä¸æ ‡å‡†çš„[w, b]
w = net[0].weight.data
print("w çš„ä¼°è®¡è¯¯å·®: ", true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print("b çš„ä¼°è®¡è¯¯å·®: ", true_b - b)
```